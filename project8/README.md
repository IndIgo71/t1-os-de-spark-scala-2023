# Проект 8. Пайплайн обработки и трансформации данных

## Задание

- **Загрузка данных в Hadoop из различных источников:**
  - Из PostgreSQL.
  - Из CSV файлов (`/var/data`).
  - Добавление колонки timestamp во все таблицы в Hadoop.
- **Создание широкой таблицы в ODS:**
  - С партиционированием по датам на каждый час.
- **Создание витрины данных:**
  - С удельными весами клиентов по категориям счетов (< 500, 500-1000, 1001-2000, 2001-3000, 3001-4000, 4001-5000, 5001-6000, > 6000) в процентном соотношении от общего числа клиентов.
- **Расчет удельного веса мужчин:**
  - В каждой категории счетов, имеющих счет в иностранном банке.
- **Выгрузка полученной витрины в PostgreSQL:**
  - Создание схемы `student45`, если она не существует.
  - Структура витрины: category, weight, mans_weight.
  - Создание не менее трех партиций за любые три часа.

## Обзор скриптов

### 1. [student45_hw8.py](airflow/dags/student45_hw08.py)
- **Назначение**: Оркестрация рабочего процесса с использованием Apache Airflow.
- **Функциональность**:
  - Планирование и управление задачами, связанными с обработкой данных.
  - Чтение данных из CSV-файлов и загрузка в HDFS, чтение из Postges по JDBC, чтение из загрузка в Hive.

### 2. [HDFSToODSLoader.scala](spark/src/main/scala/loader/HDFSToODSLoader.scala)
- **Назначение**: Загрузка CSV-файлов из HDFS в таблицу Hive.
- **Функциональность**:
  - Чтение CSV-файлов из HDFS.
  - Добавление столбца с временной меткой и запись данных в таблицу Hive.

### 3. [PgToODSLoader.scala](spark/src/main/scala/loader/PgToODSLoader.scala)
- **Назначение**: Загрузка таблиц из PostgreSQL  в Hive.
- **Функциональность**:
  - Подключение к PostgreSQL и загрузка указанных таблиц.
  - Добавление столбца с временной меткой и запись данных в таблицу Hive.

### 4. [WideTableLoader.scala](spark/src/main/scala/loader/WideTableLoader.scala)
- **Назначение**: Объединение данных из нескольких таблиц Hive в широкую таблицу.
- **Функциональность**:
  - Чтение данных из таблиц Hive (`card`, `person`, `address`, `account`).
  - Выполнение соединений и добавление нового столбца партиционирования `hour` в формате `YYYYMMDDHH24`.

### 5. [DataMartLoader.scala](spark/src/main/scala/loader/DataMartLoader.scala)
- **Назначение**: Трансформация и распределение данных в отдельную витрину в Hive.
- **Функциональность**:
  - Выполнение сложного SQL-запроса для трансформации данных.
  - Запись результата в таблицу Hive и динамически созданную таблицу PostgreSQL.

### 6. [DataMartToPGLoader.scala](spark/src/main/scala/loader/DataMartToPGLoader.scala)
- **Назначение**: Репликация витрины из Hive в PostgreSQL.
- **Функциональность**:
  - Запись результата в таблицу PostgreSQL.
  


## Требования к среде выполнения

Для запуска Scala-скриптов из этого репозитория требуется следующая конфигурация среды:

- **Airflow**: Airflow версии 2.8.1 или совместимые версии. 
- **Java**: Java 8 (1.8) или совместимые версии.
- **Scala**: Scala версии 2.11.8.
- **Spark**: Spark 2.3.2

## Настройки
- Разместите код DAG в директории /opt/airflow/dags на сервере Airflow.
- Укажите адрес PostgreSQL и другие настройки через Airflow Variables.
- Установите соответствующие соединения в Airflow для Hive и Spark.
- Запустите DAG через Airflow Web Interface или CLI.
